{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from pettingzoo.sisl import multiwalker_v7\n",
    "from collections import namedtuple, deque\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal, MultivariateNormal\n",
    "\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import argparse\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    \"\"\"Actor (Policy) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed, hidden_size=32, init_w=3e-3, log_std_min=-20, log_std_max=2):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "            fc1_units (int): Number of nodes in first hidden layer\n",
    "            fc2_units (int): Number of nodes in second hidden layer\n",
    "        \"\"\"\n",
    "        super(Actor, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.log_std_min = log_std_min\n",
    "        self.log_std_max = log_std_max\n",
    "        \n",
    "        self.fc1 = nn.Linear(state_size, hidden_size)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        \n",
    "        self.mu = nn.Linear(hidden_size, action_size)\n",
    "        self.log_std_linear = nn.Linear(hidden_size, action_size)\n",
    "\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.fc1.weight.data.uniform_(*hidden_init(self.fc1))\n",
    "        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
    "        self.mu.weight.data.uniform_(-init_w, init_w)\n",
    "        self.log_std_linear.weight.data.uniform_(-init_w, init_w)\n",
    "\n",
    "    def forward(self, state):\n",
    "\n",
    "        x = F.relu(self.fc1(state), inplace=True)\n",
    "#         x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x), inplace=True)\n",
    "        mu = self.mu(x)\n",
    "\n",
    "        log_std = self.log_std_linear(x)\n",
    "        log_std = torch.clamp(log_std, self.log_std_min, self.log_std_max)\n",
    "        return mu, log_std\n",
    "    \n",
    "    def evaluate(self, state, epsilon=1e-6):\n",
    "        mu, log_std = self.forward(state)\n",
    "        std = log_std.exp()\n",
    "        dist = Normal(0, 1)\n",
    "        e = dist.sample().to(device)\n",
    "        action = torch.tanh(mu + e * std)\n",
    "        log_prob = Normal(mu, std).log_prob(mu + e * std) - torch.log(1 - action.pow(2) + epsilon)\n",
    "\n",
    "        return action, log_prob\n",
    "        \n",
    "    \n",
    "    def get_action(self, state):\n",
    "        \"\"\"\n",
    "        returns the action based on a squashed gaussian policy. That means the samples are obtained according to:\n",
    "        a(s,e)= tanh(mu(s)+sigma(s)+e)\n",
    "        \"\"\"\n",
    "        #state = torch.FloatTensor(state).to(device) #.unsqzeeze(0)\n",
    "        mu, log_std = self.forward(state)\n",
    "        std = log_std.exp()\n",
    "        dist = Normal(0, 1)\n",
    "        e      = dist.sample().to(device)\n",
    "        action = torch.tanh(mu + e * std).cpu()\n",
    "        #action = torch.clamp(action*action_high, action_low, action_high)\n",
    "        return action[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    \"\"\"Critic (Value) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed, hidden_size=32):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "            hidden_size (int): Number of nodes in the network layers\n",
    "        \"\"\"\n",
    "        super(Critic, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fc1 = nn.Linear(state_size+action_size, hidden_size)\n",
    "#         self.dropout = nn.Dropout(0.3)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, 1)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.fc1.weight.data.uniform_(*hidden_init(self.fc1))\n",
    "        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
    "        self.fc3.weight.data.uniform_(-3e-3, 3e-3)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        \"\"\"Build a critic (value) network that maps (state, action) pairs -> Q-values.\"\"\"\n",
    "        x = torch.cat((state, action), dim=1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "#         x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hidden_init(layer):\n",
    "    fan_in = layer.weight.data.size()[0]\n",
    "    lim = 1. / np.sqrt(fan_in)\n",
    "    return (-lim, lim)\n",
    "\n",
    "class Agent():\n",
    "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
    "    \n",
    "    def __init__(self, name, state_size, action_size, random_seed, hidden_size, load_model=False, action_prior=\"uniform\"):\n",
    "        \"\"\"Initialize an Agent object.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): dimension of each state\n",
    "            action_size (int): dimension of each action\n",
    "            random_seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.name = name\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(random_seed)\n",
    "        \n",
    "        self.target_entropy = -action_size  # -dim(A)\n",
    "        self.alpha = 1\n",
    "        self.log_alpha = torch.tensor([0.0], requires_grad=True)\n",
    "        self.alpha_optimizer = optim.Adam(params=[self.log_alpha], lr=LR_ACTOR) \n",
    "        self._action_prior = action_prior\n",
    "        \n",
    "        print(\"Using: \", device)\n",
    "        \n",
    "        # Actor Network \n",
    "        self.actor_local = Actor(state_size, action_size, random_seed, hidden_size).to(device)\n",
    "        self.actor_optimizer = optim.Adam(self.actor_local.parameters(), lr=LR_ACTOR)  \n",
    "        if load_model:\n",
    "            try:\n",
    "                self.actor_local.load_state_dict(torch.load(self.name + \"_last.pt\"))\n",
    "            except Exception as e:\n",
    "                print(self.name, 'can`t load weights:', e)\n",
    "        \n",
    "        # Critic Network (w/ Target Network)\n",
    "        self.critic1 = Critic(state_size, action_size, random_seed, hidden_size).to(device)\n",
    "        self.critic2 = Critic(state_size, action_size, random_seed, hidden_size).to(device)\n",
    "        if load_model:\n",
    "            try:\n",
    "                self.critic1.load_state_dict(torch.load(self.name + \"_critic1_last.pt\"))\n",
    "                self.critic2.load_state_dict(torch.load(self.name + \"_critic1_last.pt\"))\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        self.critic1_target = Critic(state_size, action_size, random_seed,hidden_size).to(device)\n",
    "        self.critic1_target.load_state_dict(self.critic1.state_dict())\n",
    "\n",
    "        self.critic2_target = Critic(state_size, action_size, random_seed,hidden_size).to(device)\n",
    "        self.critic2_target.load_state_dict(self.critic2.state_dict())\n",
    "\n",
    "        self.critic1_optimizer = optim.Adam(self.critic1.parameters(), lr=LR_CRITIC, weight_decay=0)\n",
    "        self.critic2_optimizer = optim.Adam(self.critic2.parameters(), lr=LR_CRITIC, weight_decay=0) \n",
    "\n",
    "        # Replay memory\n",
    "        self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, random_seed)\n",
    "        \n",
    "\n",
    "    def step(self, state, action, reward, next_state, done, step):\n",
    "        \"\"\"Save experience in replay memory, and use random sample from buffer to learn.\"\"\"\n",
    "        # Save experience / reward\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "\n",
    "        # Learn, if enough samples are available in memory\n",
    "        if len(self.memory) > BATCH_SIZE:\n",
    "            experiences = self.memory.sample()\n",
    "            self.learn(step, experiences, GAMMA)\n",
    "            \n",
    "    \n",
    "    def act(self, state):\n",
    "        \"\"\"Returns actions for given state as per current policy.\"\"\"\n",
    "        state = torch.from_numpy(state).float().to(device)\n",
    "        action = self.actor_local.get_action(state).detach()\n",
    "        return action\n",
    "\n",
    "    def learn(self, step, experiences, gamma, d=1):\n",
    "        \"\"\"Updates actor, critics and entropy_alpha parameters using given batch of experience tuples.\n",
    "        Q_targets = r + γ * (min_critic_target(next_state, actor_target(next_state)) - α *log_pi(next_action|next_state))\n",
    "        Critic_loss = MSE(Q, Q_target)\n",
    "        Actor_loss = α * log_pi(a|s) - Q(s,a)\n",
    "        where:\n",
    "            actor_target(state) -> action\n",
    "            critic_target(state, action) -> Q-value\n",
    "        Params\n",
    "        ======\n",
    "            experiences (Tuple[torch.Tensor]): tuple of (s, a, r, s', done) tuples \n",
    "            gamma (float): discount factor\n",
    "        \"\"\"\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "        \n",
    "\n",
    "        # ---------------------------- update critic ---------------------------- #\n",
    "        # Get predicted next-state actions and Q values from target models\n",
    "        next_action, log_pis_next = self.actor_local.evaluate(next_states)\n",
    "\n",
    "        Q_target1_next = self.critic1_target(next_states.to(device), next_action.squeeze(0).to(device))\n",
    "        Q_target2_next = self.critic2_target(next_states.to(device), next_action.squeeze(0).to(device))\n",
    "\n",
    "        # take the mean of both critics for updating\n",
    "        Q_target_next = torch.min(Q_target1_next, Q_target2_next)\n",
    "        \n",
    "        if FIXED_ALPHA == None:\n",
    "            # Compute Q targets for current states (y_i)\n",
    "            Q_targets = rewards.cpu() + (gamma * (1 - dones.cpu()) * (Q_target_next.cpu() - self.alpha * log_pis_next.squeeze(0).cpu()))\n",
    "        else:\n",
    "            Q_targets = rewards.cpu() + (gamma * (1 - dones.cpu()) * (Q_target_next.cpu() - FIXED_ALPHA * log_pis_next.squeeze(0).cpu()))\n",
    "        # Compute critic loss\n",
    "        Q_1 = self.critic1(states, actions).cpu()\n",
    "        Q_2 = self.critic2(states, actions).cpu()\n",
    "\n",
    "        critic1_loss = 0.5*F.mse_loss(Q_1, Q_targets.detach().mean(1, keepdim=True))\n",
    "        critic2_loss = 0.5*F.mse_loss(Q_2, Q_targets.detach().mean(1, keepdim=True))\n",
    "        # Update critics\n",
    "        # critic 1\n",
    "        self.critic1_optimizer.zero_grad()\n",
    "        critic1_loss.backward()\n",
    "        self.critic1_optimizer.step()\n",
    "        # critic 2\n",
    "        self.critic2_optimizer.zero_grad()\n",
    "        critic2_loss.backward()\n",
    "        self.critic2_optimizer.step()\n",
    "        if step % d == 0:\n",
    "        # ---------------------------- update actor ---------------------------- #\n",
    "            if FIXED_ALPHA == None:\n",
    "                alpha = torch.exp(self.log_alpha)\n",
    "                # Compute alpha loss\n",
    "                actions_pred, log_pis = self.actor_local.evaluate(states)\n",
    "                alpha_loss = - (self.log_alpha.cpu() * (log_pis.cpu() + self.target_entropy).detach().cpu()).mean()\n",
    "                self.alpha_optimizer.zero_grad()\n",
    "                alpha_loss.backward()\n",
    "                self.alpha_optimizer.step()\n",
    "                \n",
    "                self.alpha = alpha\n",
    "                # Compute actor loss\n",
    "                if self._action_prior == \"normal\":\n",
    "                    policy_prior = MultivariateNormal(loc=torch.zeros(self.action_size), scale_tril=torch.ones(self.action_size).unsqueeze(0))\n",
    "                    policy_prior_log_probs = policy_prior.log_prob(actions_pred)\n",
    "                elif self._action_prior == \"uniform\":\n",
    "                    policy_prior_log_probs = 0.0\n",
    "    \n",
    "                actor_loss = (alpha * log_pis.squeeze(0).cpu() - self.critic1(states, actions_pred.squeeze(0)).cpu() - policy_prior_log_probs ).mean()\n",
    "            else:\n",
    "                \n",
    "                actions_pred, log_pis = self.actor_local.evaluate(states)\n",
    "                if self._action_prior == \"normal\":\n",
    "                    policy_prior = MultivariateNormal(loc=torch.zeros(self.action_size), scale_tril=torch.ones(self.action_size).unsqueeze(0))\n",
    "                    policy_prior_log_probs = policy_prior.log_prob(actions_pred)\n",
    "                elif self._action_prior == \"uniform\":\n",
    "                    policy_prior_log_probs = 0.0\n",
    "    \n",
    "                actor_loss = (FIXED_ALPHA * log_pis.squeeze(0).cpu() - self.critic1(states, actions_pred.squeeze(0)).cpu()- policy_prior_log_probs ).mean()\n",
    "            # Minimize the loss\n",
    "            self.actor_optimizer.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            self.actor_optimizer.step()\n",
    "\n",
    "            # ----------------------- update target networks ----------------------- #\n",
    "            self.soft_update(self.critic1, self.critic1_target, TAU)\n",
    "            self.soft_update(self.critic2, self.critic2_target, TAU)\n",
    "                     \n",
    "    \n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "        Params\n",
    "        ======\n",
    "            local_model: PyTorch model (weights will be copied from)\n",
    "            target_model: PyTorch model (weights will be copied to)\n",
    "            tau (float): interpolation parameter \n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "\n",
    "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "        Params\n",
    "        ======\n",
    "            buffer_size (int): maximum size of buffer\n",
    "            batch_size (int): size of each training batch\n",
    "        \"\"\"\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)  # internal memory (deque)\n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "        self.prob = []\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "    \n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "        \n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).float().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "\n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dic_builder(num_walkers, value):\n",
    "    return {f'walker_{n}':value for n in range(num_walkers)}\n",
    "\n",
    "def SAC(n_episodes=200, max_t=500, print_every=10):\n",
    "    for ag in agent:\n",
    "        ag.actor_local.train()\n",
    "    scores_deque = deque(maxlen=50)\n",
    "    average_100_scores = []\n",
    "    best_score = BEST_SCORE\n",
    "\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "\n",
    "        env.reset()\n",
    "        state = dic_builder(n_walkers, np.zeros((1, 31)))\n",
    "        action = dic_builder(n_walkers, np.zeros((4)))\n",
    "        reward = dic_builder(n_walkers, 0)\n",
    "        next_state = dic_builder(n_walkers, np.zeros((1, 31)))\n",
    "        done = dic_builder(n_walkers, False)\n",
    "        score = 0\n",
    "        for t, wlkr in enumerate(env.agent_iter()):\n",
    "            next_state[wlkr], reward[wlkr], done[wlkr], _ = env.last()\n",
    "            next_state[wlkr] = next_state[wlkr].reshape((1,state_size))\n",
    "            \n",
    "            agent[t%n_walkers].step(state[wlkr], action[wlkr], reward[wlkr], next_state[wlkr], done[wlkr], int(t/n_walkers))\n",
    "            \n",
    "            if done[wlkr]:\n",
    "                if all(done):\n",
    "                    break\n",
    "                env.step(None)\n",
    "                continue\n",
    "                \n",
    "            state[wlkr] = next_state[wlkr].reshape((1,state_size))\n",
    "            action[wlkr] = agent[t%n_walkers].act(state[wlkr])\n",
    "            action_v = action[wlkr].numpy()\n",
    "            action_v = np.clip(action_v*action_high, action_low, action_high)\n",
    "            env.step(action_v)\n",
    "            \n",
    "            score += reward[wlkr] / n_walkers\n",
    "        \n",
    "        scores_deque.append(score)\n",
    "        average_100_scores.append(np.mean(scores_deque))\n",
    "        if average_100_scores[-1] > best_score:\n",
    "            best_score = average_100_scores[-1]\n",
    "#             if last_name != None:\n",
    "#                 !del $last_name\n",
    "#             last_name = agent.name + f\"_Best_{best_score:.1f}.pt\"\n",
    "#             torch.save(agent.actor_local.state_dict(), last_name)\n",
    "        \n",
    "        print('\\rEpisode {} Reward: {:.2f}  Average50 Score: {:.2f} best: {:.2f}'.format(i_episode, score, np.mean(scores_deque), best_score), \n",
    "              ' '*10, end=\"\")\n",
    "        if i_episode % print_every == 0:\n",
    "            print('\\rEpisode {}  Reward: {:.2f}  Average50 Score: {:.2f} best: {:.2f}'.format(i_episode, score, np.mean(scores_deque), best_score), \n",
    "                  ' '*10)\n",
    "    for ag in agent: \n",
    "        torch.save(ag.actor_local.state_dict(), ag.name + \"_last.pt\")\n",
    "        torch.save(ag.critic1.state_dict(), ag.name + \"_critic1_last.pt\")\n",
    "        torch.save(ag.critic2.state_dict(), ag.name + \"_critic2_last.pt\")\n",
    "        print('\\nmodel saved', ag.name + \"_score_last.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play(max_t=1000, graphics=False, save_video=False):\n",
    "    if save_video:\n",
    "        from gym.wrappers import Monitor\n",
    "        env = Monitor(multiwalker_v7.env(n_walkers=2, max_cycles=300, forward_reward=1.0), './video', force=True)\n",
    "    else:\n",
    "        env = multiwalker_v7.env(n_walkers=n_walkers, max_cycles=300, forward_reward=1.0)\n",
    "    for i in range(n_walkers):\n",
    "        agent[i].actor_local.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i_episode in range(3):\n",
    "            \n",
    "            env.reset()\n",
    "            state = dic_builder(n_walkers, np.zeros((1, 31)))\n",
    "            action = dic_builder(n_walkers, np.zeros((4)))\n",
    "            reward = dic_builder(n_walkers, 0)\n",
    "            next_state = dic_builder(n_walkers, np.zeros((1, 31)))\n",
    "            done = dic_builder(n_walkers, False)\n",
    "            score = 0\n",
    "            \n",
    "            for t, wlkr in enumerate(env.agent_iter()):\n",
    "                state[wlkr], reward[wlkr], done[wlkr], _ = env.last()\n",
    "                if graphics:\n",
    "                    env.render()\n",
    "                if done[wlkr]:\n",
    "                    if all(done):\n",
    "                        env.close()\n",
    "                        break\n",
    "                    env.step(None)\n",
    "                    continue\n",
    "\n",
    "                state[wlkr] = state[wlkr].reshape((1,state_size))\n",
    "                action[wlkr] = agent[t%n_walkers].act(state[wlkr])\n",
    "                action_v = action[wlkr].numpy()\n",
    "                action_v = np.clip(action_v*action_high, action_low, action_high)\n",
    "                env.step(action_v)\n",
    "\n",
    "                score += reward[wlkr] / n_walkers\n",
    "            \n",
    "            print(f'{score = :.2f}')\n",
    "            env.reset()\n",
    "\n",
    "        env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 0\n",
    "PRINT_EVERY = 100\n",
    "BEST_SCORE = 0\n",
    "GAMMA = 0.99\n",
    "TAU = 1e-2\n",
    "HIDDEN_SIZE = 256\n",
    "BUFFER_SIZE = int(5e5) #int(1e6)\n",
    "BATCH_SIZE = 256\n",
    "LR_ACTOR = 4e-4\n",
    "LR_CRITIC = 4e-4\n",
    "FIXED_ALPHA = None #\"entropy alpha value, if not choosen the value is leaned by the agent\"\n",
    "\n",
    "n_walkers = 2\n",
    "N_EPOCHS = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using:  cuda:0\n",
      "Using:  cuda:0\n",
      "Episode 100  Reward: -4.23  Average50 Score: -1.92 best: 7.00           \n",
      "Episode 200  Reward: -13.17  Average50 Score: -2.46 best: 7.00           \n",
      "Episode 300  Reward: -2.53  Average50 Score: -2.02 best: 7.00           \n",
      "Episode 400  Reward: -3.67  Average50 Score: -1.65 best: 7.00           \n",
      "Episode 500  Reward: -9.85  Average50 Score: -7.48 best: 7.00           \n",
      "Episode 600  Reward: -16.71  Average50 Score: -2.70 best: 7.00           \n",
      "Episode 700  Reward: -4.91  Average50 Score: -6.52 best: 7.00           \n",
      "Episode 800  Reward: 1.27  Average50 Score: -0.36 best: 7.00            \n",
      "Episode 900  Reward: -4.74  Average50 Score: -3.21 best: 7.00           \n",
      "Episode 992 Reward: -3.29  Average50 Score: -6.16 best: 7.00            "
     ]
    }
   ],
   "source": [
    "train = True\n",
    "load_model = True\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    t0 = time.time()\n",
    "    env = multiwalker_v7.env(n_walkers=n_walkers, max_cycles=300, forward_reward=1.0)\n",
    "    action_high = env.action_spaces['walker_0'].high[0]\n",
    "    action_low = env.action_spaces['walker_0'].low[0]\n",
    "    torch.manual_seed(SEED)\n",
    "    env.seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    state_size = env.observation_spaces['walker_0'].shape[0]\n",
    "    action_size = env.action_spaces['walker_0'].shape[0]\n",
    "    agent = []\n",
    "    for i in range(n_walkers):\n",
    "        agent.append(Agent(name='walker_'+str(i), state_size=state_size, action_size=action_size, \n",
    "                           random_seed=SEED,hidden_size=HIDDEN_SIZE, load_model=load_model, action_prior=\"uniform\")) #\"normal\"\n",
    "    \n",
    "    \n",
    "    if train:\n",
    "        SAC(n_episodes=N_EPOCHS, max_t=300, print_every=PRINT_EVERY)\n",
    "    else:\n",
    "        play(1000, True, False)\n",
    "    \n",
    "    t1 = time.time()\n",
    "    env.close()\n",
    "    print(\"training took {} min!\".format((t1-t0)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.actor_local.load_state_dict(torch.load('BipedalWalker_HC_score_last.pt'))\n",
    "play(300, test=True, save_video=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!start ."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
